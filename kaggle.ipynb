{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219b2d21-547a-464c-9a7d-b8afdeddaf18",
   "metadata": {},
   "source": [
    "# DM2024 Lab 2 Homework: Emotion Recognition on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d940123-e8f1-4847-bd3a-b4396acef952",
   "metadata": {},
   "source": [
    "## Objective\n",
    "The goal of this notebook is to develop a machine learning model that predicts the emotion behind tweets from the provided dataset. This involves:\n",
    "- Cleaning and preprocessing the text data.\n",
    "- Engineering meaningful features.\n",
    "- Experimenting with different models and evaluating their performance.\n",
    "- Submitting predictions in the required format for the Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49540d-8737-4402-b8a4-0909227b7beb",
   "metadata": {},
   "source": [
    "Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e99df09-3f85-466e-8031-20c23cc153f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim: 4.3.3\n",
      "tensorflow: 2.18.0\n",
      "keras: 3.6.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import gensim\n",
    "import tensorflow\n",
    "import keras\n",
    "import ollama\n",
    "import langchain\n",
    "import langchain_community\n",
    "import langchain_core\n",
    "import bs4\n",
    "import chromadb\n",
    "import gradio\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"gensim: \" + gensim.__version__)\n",
    "print(\"tensorflow: \" + tensorflow.__version__)\n",
    "print(\"keras: \" + keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402fb8b4-9282-4ce2-bb09-6d496606443d",
   "metadata": {},
   "source": [
    "Data Loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c845927-275a-4a53-88d4-7cf993e8724f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets Data:\n",
      "   _score          _index                                            _source  \\\n",
      "0     391  hashtag_tweets  {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
      "1     433  hashtag_tweets  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
      "2     232  hashtag_tweets  {'tweet': {'hashtags': ['bibleverse'], 'tweet_...   \n",
      "3     376  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
      "4     989  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x2de2...   \n",
      "\n",
      "            _crawldate   _type  \n",
      "0  2015-05-23 11:42:47  tweets  \n",
      "1  2016-01-28 04:52:09  tweets  \n",
      "2  2017-12-25 04:39:20  tweets  \n",
      "3  2016-01-24 23:53:05  tweets  \n",
      "4  2016-01-08 17:18:59  tweets  \n",
      "\n",
      "Emotion Data:\n",
      "   tweet_id       emotion\n",
      "0  0x3140b1       sadness\n",
      "1  0x368b73       disgust\n",
      "2  0x296183  anticipation\n",
      "3  0x2bd6e1           joy\n",
      "4  0x2ee1dd  anticipation\n",
      "\n",
      "Data Identification:\n",
      "   tweet_id identification\n",
      "0  0x28cc61           test\n",
      "1  0x29e452          train\n",
      "2  0x2b3819          train\n",
      "3  0x2db41f           test\n",
      "4  0x2a2acc          train\n",
      "\n",
      "Sample Submission:\n",
      "         id   emotion\n",
      "0  0x2c7743  surprise\n",
      "1  0x2c1eed  surprise\n",
      "2  0x2826ea  surprise\n",
      "3  0x356d9a  surprise\n",
      "4  0x20fd95  surprise\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "base_path = Path(r'C:\\Users\\katy6\\dm-2024-isa-5810-lab-2-homework (2)')\n",
    "tweets_df = pd.read_json(base_path / 'tweets_DM.json', lines=True)\n",
    "emotion_df = pd.read_csv(base_path / 'emotion.csv')  # 含情緒標籤\n",
    "data_identification_df = pd.read_csv(base_path / 'data_identification.csv')  # 訓練/測試區分\n",
    "sample_submission_df = pd.read_csv(base_path / 'sampleSubmission.csv')  # 提交模板\n",
    "# 檢查每個數據集的前幾行\n",
    "print(\"Tweets Data:\")\n",
    "print(tweets_df.head())\n",
    "\n",
    "print(\"\\nEmotion Data:\")\n",
    "print(emotion_df.head())\n",
    "\n",
    "print(\"\\nData Identification:\")\n",
    "print(data_identification_df.head())\n",
    "\n",
    "print(\"\\nSample Submission:\")\n",
    "print(sample_submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b71480c-0780-43a2-8ae4-fdaa6818d867",
   "metadata": {},
   "source": [
    "Purpose: This function cleans raw tweet text by:\n",
    "Removing URLs, mentions, hashtags, and punctuation.\n",
    "Converting text to lowercase.\n",
    "Tokenizing the text into individual words.\n",
    "Lemmatizing the words (reducing them to their base form).\n",
    "Removing stopwords to focus on meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de6202-d5ea-4ba8-a8c8-be367a7b7dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\katy6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\katy6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\katy6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# 確保必要的 NLTK 資源已下載\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 讀取數據\n",
    "tweets_df = pd.read_json(r'C:\\Users\\katy6\\dm-2024-isa-5810-lab-2-homework (2)\\tweets_DM.json', lines=True)\n",
    "emotion_df = pd.read_csv(r'C:\\Users\\katy6\\dm-2024-isa-5810-lab-2-homework (2)\\emotion.csv')  # 含情緒標籤\n",
    "data_identification_df = pd.read_csv(r'C:\\Users\\katy6\\dm-2024-isa-5810-lab-2-homework (2)\\data_identification.csv')  # 訓練/測試區分\n",
    "\n",
    "# 提取推文的 tweet_id 和 text\n",
    "tweets_df['tweet_id'] = tweets_df['_source'].apply(lambda x: x['tweet']['tweet_id'])\n",
    "tweets_df['text'] = tweets_df['_source'].apply(lambda x: x['tweet']['text'])\n",
    "\n",
    "# 合併數據\n",
    "merged_df = data_identification_df.merge(tweets_df[['tweet_id', 'text']], on='tweet_id', how='left')\n",
    "merged_df = merged_df.merge(emotion_df, on='tweet_id', how='left')\n",
    "\n",
    "# 定義清理文本函數\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # 移除網址\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  # 移除提及\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)  # 移除Hashtags\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # 移除標點符號\n",
    "    text = text.lower()  # 全部轉小寫\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# 應用清理函數\n",
    "merged_df['cleaned_text'] = merged_df['text'].apply(clean_text)\n",
    "\n",
    "# 查看清理後的結果\n",
    "print(merged_df[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e40ea7c-30fc-428c-bd27-2b1145d12f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet': {'hashtags': ['Snapchat'], 'tweet_id': '0x376b20', 'text': 'People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that\\'s <LH>'}}\n"
     ]
    }
   ],
   "source": [
    "# 檢查 _source 結構\n",
    "print(tweets_df['_source'].iloc[0])  # 查看第一行的 _source 結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63bb4b4f-60bf-4428-b9fe-921fd5275d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['tweet_id'] = tweets_df['_source'].apply(lambda x: x.get('tweet', {}).get('tweet_id', ''))\n",
    "tweets_df['text'] = tweets_df['_source'].apply(lambda x: x.get('tweet', {}).get('text', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2d9ce1e-8385-48f2-8784-4971fbee9df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Check out this amazing #tutorial on @Twitter: https://example.com!\n",
      "Cleaned: check amazing\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Check out this amazing #tutorial on @Twitter: https://example.com!\"\n",
    "cleaned = clean_text(test_text)\n",
    "print(\"Original:\", test_text)\n",
    "print(\"Cleaned:\", cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfc53849-b1fd-4aa8-aa9f-c1d16640d1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  @Habbo I've seen two separate colours of the e...   \n",
      "1  Huge Respect🖒 @JohnnyVegasReal talking about l...   \n",
      "2  Yoooo we hit all our monthly goals with the ne...   \n",
      "3  @FoxNews @KellyannePolls No serious self respe...   \n",
      "4  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  ive seen two separate colour elegant furni hom...  \n",
      "1  huge respect talking losing dad cancerif dont ...  \n",
      "2         yoooo hit monthly goal new app two week lh  \n",
      "3  serious self respecting individual belief much...  \n",
      "4                        well done team lh every one  \n"
     ]
    }
   ],
   "source": [
    "print(merged_df[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56272f0d-7f39-4386-8311-2be26e40b986",
   "metadata": {},
   "source": [
    "Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "315d18c1-2f5d-4272-9a9b-50bca9eadeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 分離訓練和測試數據\n",
    "train_data = merged_df[merged_df['identification'] == 'train']\n",
    "test_data = merged_df[merged_df['identification'] == 'test']\n",
    "\n",
    "# TF-IDF 向量化\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform(train_data['cleaned_text'])\n",
    "X_test = vectorizer.transform(test_data['cleaned_text'])\n",
    "\n",
    "# 提取訓練數據的情緒標籤\n",
    "y_train = train_data['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9151e0f7-a34d-498c-ad49-8b260eb78610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\katy6\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.57      0.12      0.20     39867\n",
      "anticipation       0.54      0.44      0.48    248935\n",
      "     disgust       0.38      0.25      0.30    139101\n",
      "        fear       0.63      0.24      0.34     63999\n",
      "         joy       0.47      0.81      0.60    516017\n",
      "     sadness       0.40      0.34      0.37    193437\n",
      "    surprise       0.67      0.11      0.19     48729\n",
      "       trust       0.50      0.18      0.26    205478\n",
      "\n",
      "    accuracy                           0.47   1455563\n",
      "   macro avg       0.52      0.31      0.34   1455563\n",
      "weighted avg       0.48      0.47      0.44   1455563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 訓練模型\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 評估模型\n",
    "y_pred_train = model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdd2b62e-b413-41b4-9af2-b347458f54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.copy()\n",
    "test_data['emotion'] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e542c10-3ac4-4e76-96a2-d33ce987001f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提交文件已成功生成：submission.csv\n"
     ]
    }
   ],
   "source": [
    "# 預測測試集情緒\n",
    "test_data['emotion'] = model.predict(X_test)\n",
    "\n",
    "# 生成提交文件\n",
    "submission = test_data[['tweet_id', 'emotion']]\n",
    "submission.columns = ['id', 'emotion']\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"提交文件已成功生成：submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
