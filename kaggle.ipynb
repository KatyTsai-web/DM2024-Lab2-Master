{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219b2d21-547a-464c-9a7d-b8afdeddaf18",
   "metadata": {},
   "source": [
    "# DM2024 Lab 2 Homework: Emotion Recognition on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d940123-e8f1-4847-bd3a-b4396acef952",
   "metadata": {},
   "source": [
    "## Objective\n",
    "The goal of this notebook is to develop a machine learning model that predicts the emotion behind tweets from the provided dataset. This involves:\n",
    "- Cleaning and preprocessing the text data.\n",
    "- Engineering meaningful features.\n",
    "- Experimenting with different models and evaluating their performance.\n",
    "- Submitting predictions in the required format for the Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49540d-8737-4402-b8a4-0909227b7beb",
   "metadata": {},
   "source": [
    "Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e99df09-3f85-466e-8031-20c23cc153f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim: 4.3.3\n",
      "tensorflow: 2.18.0\n",
      "keras: 3.6.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import gensim\n",
    "import tensorflow\n",
    "import keras\n",
    "import ollama\n",
    "import langchain\n",
    "import langchain_community\n",
    "import langchain_core\n",
    "import bs4\n",
    "import chromadb\n",
    "import gradio\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"gensim: \" + gensim.__version__)\n",
    "print(\"tensorflow: \" + tensorflow.__version__)\n",
    "print(\"keras: \" + keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402fb8b4-9282-4ce2-bb09-6d496606443d",
   "metadata": {},
   "source": [
    "Data Loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c845927-275a-4a53-88d4-7cf993e8724f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets Data:\n",
      "   _score          _index                                            _source  \\\n",
      "0     391  hashtag_tweets  {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
      "1     433  hashtag_tweets  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
      "2     232  hashtag_tweets  {'tweet': {'hashtags': ['bibleverse'], 'tweet_...   \n",
      "3     376  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
      "4     989  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x2de2...   \n",
      "\n",
      "            _crawldate   _type  \n",
      "0  2015-05-23 11:42:47  tweets  \n",
      "1  2016-01-28 04:52:09  tweets  \n",
      "2  2017-12-25 04:39:20  tweets  \n",
      "3  2016-01-24 23:53:05  tweets  \n",
      "4  2016-01-08 17:18:59  tweets  \n",
      "\n",
      "Emotion Data:\n",
      "   tweet_id       emotion\n",
      "0  0x3140b1       sadness\n",
      "1  0x368b73       disgust\n",
      "2  0x296183  anticipation\n",
      "3  0x2bd6e1           joy\n",
      "4  0x2ee1dd  anticipation\n",
      "\n",
      "Data Identification:\n",
      "   tweet_id identification\n",
      "0  0x28cc61           test\n",
      "1  0x29e452          train\n",
      "2  0x2b3819          train\n",
      "3  0x2db41f           test\n",
      "4  0x2a2acc          train\n",
      "\n",
      "Sample Submission:\n",
      "         id   emotion\n",
      "0  0x2c7743  surprise\n",
      "1  0x2c1eed  surprise\n",
      "2  0x2826ea  surprise\n",
      "3  0x356d9a  surprise\n",
      "4  0x20fd95  surprise\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "base_path = Path(r'C:\\Users\\katy6\\dm-2024-isa-5810-lab-2-homework (2)')\n",
    "tweets_df = pd.read_json(base_path / 'tweets_DM.json', lines=True)\n",
    "emotion_df = pd.read_csv(base_path / 'emotion.csv')  # å«æƒ…ç·’æ¨™ç±¤\n",
    "data_identification_df = pd.read_csv(base_path / 'data_identification.csv')  # è¨“ç·´/æ¸¬è©¦å€åˆ†\n",
    "sample_submission_df = pd.read_csv(base_path / 'sampleSubmission.csv')  # æäº¤æ¨¡æ¿\n",
    "# æª¢æŸ¥æ¯å€‹æ•¸æ“šé›†çš„å‰å¹¾è¡Œ\n",
    "print(\"Tweets Data:\")\n",
    "print(tweets_df.head())\n",
    "\n",
    "print(\"\\nEmotion Data:\")\n",
    "print(emotion_df.head())\n",
    "\n",
    "print(\"\\nData Identification:\")\n",
    "print(data_identification_df.head())\n",
    "\n",
    "print(\"\\nSample Submission:\")\n",
    "print(sample_submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b71480c-0780-43a2-8ae4-fdaa6818d867",
   "metadata": {},
   "source": [
    "Purpose: This function cleans raw tweet text by:\n",
    "Removing URLs, mentions, hashtags, and punctuation.\n",
    "Converting text to lowercase.\n",
    "Tokenizing the text into individual words.\n",
    "Lemmatizing the words (reducing them to their base form).\n",
    "Removing stopwords to focus on meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de6202-d5ea-4ba8-a8c8-be367a7b7dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\katy6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\katy6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\katy6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# ç¢ºä¿å¿…è¦çš„ NLTK è³‡æºå·²ä¸‹è¼‰\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# è®€å–æ•¸æ“š\n",
    "tweets_df = pd.read_json(r'C:\\Users\\katy6\\dm-2024-isa-5810-lab-2-homework (2)\\tweets_DM.json', lines=True)\n",
    "emotion_df = pd.read_csv(r'C:\\Users\\katy6\\dm-2024-isa-5810-lab-2-homework (2)\\emotion.csv')  # å«æƒ…ç·’æ¨™ç±¤\n",
    "data_identification_df = pd.read_csv(r'C:\\Users\\katy6\\dm-2024-isa-5810-lab-2-homework (2)\\data_identification.csv')  # è¨“ç·´/æ¸¬è©¦å€åˆ†\n",
    "\n",
    "# æå–æ¨æ–‡çš„ tweet_id å’Œ text\n",
    "tweets_df['tweet_id'] = tweets_df['_source'].apply(lambda x: x['tweet']['tweet_id'])\n",
    "tweets_df['text'] = tweets_df['_source'].apply(lambda x: x['tweet']['text'])\n",
    "\n",
    "# åˆä½µæ•¸æ“š\n",
    "merged_df = data_identification_df.merge(tweets_df[['tweet_id', 'text']], on='tweet_id', how='left')\n",
    "merged_df = merged_df.merge(emotion_df, on='tweet_id', how='left')\n",
    "\n",
    "# å®šç¾©æ¸…ç†æ–‡æœ¬å‡½æ•¸\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # ç§»é™¤ç¶²å€\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  # ç§»é™¤æåŠ\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)  # ç§»é™¤Hashtags\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # ç§»é™¤æ¨™é»ç¬¦è™Ÿ\n",
    "    text = text.lower()  # å…¨éƒ¨è½‰å°å¯«\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# æ‡‰ç”¨æ¸…ç†å‡½æ•¸\n",
    "merged_df['cleaned_text'] = merged_df['text'].apply(clean_text)\n",
    "\n",
    "# æŸ¥çœ‹æ¸…ç†å¾Œçš„çµæœ\n",
    "print(merged_df[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e40ea7c-30fc-428c-bd27-2b1145d12f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet': {'hashtags': ['Snapchat'], 'tweet_id': '0x376b20', 'text': 'People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that\\'s <LH>'}}\n"
     ]
    }
   ],
   "source": [
    "# æª¢æŸ¥ _source çµæ§‹\n",
    "print(tweets_df['_source'].iloc[0])  # æŸ¥çœ‹ç¬¬ä¸€è¡Œçš„ _source çµæ§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63bb4b4f-60bf-4428-b9fe-921fd5275d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['tweet_id'] = tweets_df['_source'].apply(lambda x: x.get('tweet', {}).get('tweet_id', ''))\n",
    "tweets_df['text'] = tweets_df['_source'].apply(lambda x: x.get('tweet', {}).get('text', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2d9ce1e-8385-48f2-8784-4971fbee9df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Check out this amazing #tutorial on @Twitter: https://example.com!\n",
      "Cleaned: check amazing\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Check out this amazing #tutorial on @Twitter: https://example.com!\"\n",
    "cleaned = clean_text(test_text)\n",
    "print(\"Original:\", test_text)\n",
    "print(\"Cleaned:\", cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfc53849-b1fd-4aa8-aa9f-c1d16640d1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  @Habbo I've seen two separate colours of the e...   \n",
      "1  Huge RespectğŸ–’ @JohnnyVegasReal talking about l...   \n",
      "2  Yoooo we hit all our monthly goals with the ne...   \n",
      "3  @FoxNews @KellyannePolls No serious self respe...   \n",
      "4  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  ive seen two separate colour elegant furni hom...  \n",
      "1  huge respect talking losing dad cancerif dont ...  \n",
      "2         yoooo hit monthly goal new app two week lh  \n",
      "3  serious self respecting individual belief much...  \n",
      "4                        well done team lh every one  \n"
     ]
    }
   ],
   "source": [
    "print(merged_df[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56272f0d-7f39-4386-8311-2be26e40b986",
   "metadata": {},
   "source": [
    "Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "315d18c1-2f5d-4272-9a9b-50bca9eadeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# åˆ†é›¢è¨“ç·´å’Œæ¸¬è©¦æ•¸æ“š\n",
    "train_data = merged_df[merged_df['identification'] == 'train']\n",
    "test_data = merged_df[merged_df['identification'] == 'test']\n",
    "\n",
    "# TF-IDF å‘é‡åŒ–\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform(train_data['cleaned_text'])\n",
    "X_test = vectorizer.transform(test_data['cleaned_text'])\n",
    "\n",
    "# æå–è¨“ç·´æ•¸æ“šçš„æƒ…ç·’æ¨™ç±¤\n",
    "y_train = train_data['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9151e0f7-a34d-498c-ad49-8b260eb78610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\katy6\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.57      0.12      0.20     39867\n",
      "anticipation       0.54      0.44      0.48    248935\n",
      "     disgust       0.38      0.25      0.30    139101\n",
      "        fear       0.63      0.24      0.34     63999\n",
      "         joy       0.47      0.81      0.60    516017\n",
      "     sadness       0.40      0.34      0.37    193437\n",
      "    surprise       0.67      0.11      0.19     48729\n",
      "       trust       0.50      0.18      0.26    205478\n",
      "\n",
      "    accuracy                           0.47   1455563\n",
      "   macro avg       0.52      0.31      0.34   1455563\n",
      "weighted avg       0.48      0.47      0.44   1455563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# è¨“ç·´æ¨¡å‹\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# è©•ä¼°æ¨¡å‹\n",
    "y_pred_train = model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdd2b62e-b413-41b4-9af2-b347458f54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.copy()\n",
    "test_data['emotion'] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e542c10-3ac4-4e76-96a2-d33ce987001f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æäº¤æ–‡ä»¶å·²æˆåŠŸç”Ÿæˆï¼šsubmission.csv\n"
     ]
    }
   ],
   "source": [
    "# é æ¸¬æ¸¬è©¦é›†æƒ…ç·’\n",
    "test_data['emotion'] = model.predict(X_test)\n",
    "\n",
    "# ç”Ÿæˆæäº¤æ–‡ä»¶\n",
    "submission = test_data[['tweet_id', 'emotion']]\n",
    "submission.columns = ['id', 'emotion']\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"æäº¤æ–‡ä»¶å·²æˆåŠŸç”Ÿæˆï¼šsubmission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
